{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hw4prob3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PGnlRWvkY-2c"
      },
      "source": [
        "In this problem we will use the BERT model for sentiment analysis. We will start with a pre-trained BERT model and fine-tune it on a dataset of Google Play store reviews."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wmj22-TcZMef"
      },
      "source": [
        "## Setup\n",
        "\n",
        "Install [the Transformers library](https://huggingface.co/transformers/) by Hugging Face:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kj_7Tz0-pK69"
      },
      "source": [
        "!pip install -q -U watermark"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jjsbi1u3QFEM"
      },
      "source": [
        "!pip install -qq transformers"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AJqoaFpVpoM8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df72df66-6acb-468f-cf26-21fad841f450"
      },
      "source": [
        "%reload_ext watermark\n",
        "%watermark -v -p numpy,pandas,torch,transformers"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Python implementation: CPython\n",
            "Python version       : 3.7.10\n",
            "IPython version      : 5.5.0\n",
            "\n",
            "numpy       : 1.19.5\n",
            "pandas      : 1.1.5\n",
            "torch       : 1.8.1+cu101\n",
            "transformers: 4.4.2\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ufzPdoTtNikq"
      },
      "source": [
        "## Data Exploration\n",
        "\n",
        "Download the Google Play app reviews dataset using the following commands:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SgPRhuMzi9ot",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7cd1452-990a-4535-dae4-ae1772713258"
      },
      "source": [
        "!gdown --id 1S6qMioqPJjyBLpLVz4gmRTnJHnjitnuV\n",
        "!gdown --id 1zdmewp7ayS4js4VtrJEHzAheSW-5NBZv"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1S6qMioqPJjyBLpLVz4gmRTnJHnjitnuV\n",
            "To: /content/apps.csv\n",
            "100% 134k/134k [00:00<00:00, 59.6MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1zdmewp7ayS4js4VtrJEHzAheSW-5NBZv\n",
            "To: /content/reviews.csv\n",
            "7.17MB [00:00, 112MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S7GO8vXo6IVO"
      },
      "source": [
        "Here is how it looks like:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mUKLyKc7I6Qp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "907e6635-aabe-447a-c208-22c043161da1"
      },
      "source": [
        "import pandas as pd \n",
        "df = pd.read_csv(\"reviews.csv\")\n",
        "df.head()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>userName</th>\n",
              "      <th>userImage</th>\n",
              "      <th>content</th>\n",
              "      <th>score</th>\n",
              "      <th>thumbsUpCount</th>\n",
              "      <th>reviewCreatedVersion</th>\n",
              "      <th>at</th>\n",
              "      <th>replyContent</th>\n",
              "      <th>repliedAt</th>\n",
              "      <th>sortOrder</th>\n",
              "      <th>appId</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Andrew Thomas</td>\n",
              "      <td>https://lh3.googleusercontent.com/a-/AOh14GiHd...</td>\n",
              "      <td>Update: After getting a response from the deve...</td>\n",
              "      <td>1</td>\n",
              "      <td>21</td>\n",
              "      <td>4.17.0.3</td>\n",
              "      <td>2020-04-05 22:25:57</td>\n",
              "      <td>According to our TOS, and the term you have ag...</td>\n",
              "      <td>2020-04-05 15:10:24</td>\n",
              "      <td>most_relevant</td>\n",
              "      <td>com.anydo</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Craig Haines</td>\n",
              "      <td>https://lh3.googleusercontent.com/-hoe0kwSJgPQ...</td>\n",
              "      <td>Used it for a fair amount of time without any ...</td>\n",
              "      <td>1</td>\n",
              "      <td>11</td>\n",
              "      <td>4.17.0.3</td>\n",
              "      <td>2020-04-04 13:40:01</td>\n",
              "      <td>It sounds like you logged in with a different ...</td>\n",
              "      <td>2020-04-05 15:11:35</td>\n",
              "      <td>most_relevant</td>\n",
              "      <td>com.anydo</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>steven adkins</td>\n",
              "      <td>https://lh3.googleusercontent.com/a-/AOh14GiXw...</td>\n",
              "      <td>Your app sucks now!!!!! Used to be good but no...</td>\n",
              "      <td>1</td>\n",
              "      <td>17</td>\n",
              "      <td>4.17.0.3</td>\n",
              "      <td>2020-04-01 16:18:13</td>\n",
              "      <td>This sounds odd! We are not aware of any issue...</td>\n",
              "      <td>2020-04-02 16:05:56</td>\n",
              "      <td>most_relevant</td>\n",
              "      <td>com.anydo</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Lars Panzerbjørn</td>\n",
              "      <td>https://lh3.googleusercontent.com/a-/AOh14Gg-h...</td>\n",
              "      <td>It seems OK, but very basic. Recurring tasks n...</td>\n",
              "      <td>1</td>\n",
              "      <td>192</td>\n",
              "      <td>4.17.0.2</td>\n",
              "      <td>2020-03-12 08:17:34</td>\n",
              "      <td>We do offer this option as part of the Advance...</td>\n",
              "      <td>2020-03-15 06:20:13</td>\n",
              "      <td>most_relevant</td>\n",
              "      <td>com.anydo</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Scott Prewitt</td>\n",
              "      <td>https://lh3.googleusercontent.com/-K-X1-YsVd6U...</td>\n",
              "      <td>Absolutely worthless. This app runs a prohibit...</td>\n",
              "      <td>1</td>\n",
              "      <td>42</td>\n",
              "      <td>4.17.0.2</td>\n",
              "      <td>2020-03-14 17:41:01</td>\n",
              "      <td>We're sorry you feel this way! 90% of the app ...</td>\n",
              "      <td>2020-03-15 23:45:51</td>\n",
              "      <td>most_relevant</td>\n",
              "      <td>com.anydo</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           userName  ...      appId\n",
              "0     Andrew Thomas  ...  com.anydo\n",
              "1      Craig Haines  ...  com.anydo\n",
              "2     steven adkins  ...  com.anydo\n",
              "3  Lars Panzerbjørn  ...  com.anydo\n",
              "4     Scott Prewitt  ...  com.anydo\n",
              "\n",
              "[5 rows x 11 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9AiAdQ3j6SDe"
      },
      "source": [
        "Let's first check the size of the dataset. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dB2jE6am7Dpo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36d573fb-9c7a-44b9-d408-465331afd057"
      },
      "source": [
        "# TODO: Q1. How many samples are there in this dataset? \n",
        "data = df.values\n",
        "print(data.shape)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(15746, 11)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uYZ4wOnwKfGt"
      },
      "source": [
        "There are 15746 samples."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wwh_rW4Efhs3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        },
        "outputId": "ce6edbff-0822-4ba4-e9ca-6ef206c2ce74"
      },
      "source": [
        "# TODO: Q2. Plot a histogram of review scores. These can be accessed in the df.score field in the above dataframe. Which score is the most common?\n",
        "import matplotlib.pyplot as plt\n",
        "# print(df[1][3])\n",
        "plt.hist(data[:,3])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([2566.,    0., 2462.,    0.,    0., 5042.,    0., 2776.,    0.,\n",
              "        2900.]), array([1.0, 1.4, 1.8, 2.2, 2.6, 3.0, 3.4000000000000004,\n",
              "        3.8000000000000003, 4.2, 4.6, 5.0], dtype=object), <a list of 10 Patch objects>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tRNZXomoKWTz"
      },
      "source": [
        "We can see the score around 3.0 is the most common."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nZM0GKviobjM"
      },
      "source": [
        "If correctly plotted, you should be able to see that this is a somewhat imbalanced dataset. Let's first convert the dataset into three classes: negative, neutral, and positive sentiment:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ei0xmdi1Chp0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "2eb6e646-2917-47bb-fef1-d5fb55d8ca72"
      },
      "source": [
        "def to_sentiment(rating):\n",
        "  rating = int(rating)\n",
        "  if rating <= 2:\n",
        "    return 0\n",
        "  elif rating == 3:\n",
        "    return 1\n",
        "  else: \n",
        "    return 2\n",
        "\n",
        "df['sentiment'] = df.score.apply(to_sentiment)\n",
        "df.head()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>userName</th>\n",
              "      <th>userImage</th>\n",
              "      <th>content</th>\n",
              "      <th>score</th>\n",
              "      <th>thumbsUpCount</th>\n",
              "      <th>reviewCreatedVersion</th>\n",
              "      <th>at</th>\n",
              "      <th>replyContent</th>\n",
              "      <th>repliedAt</th>\n",
              "      <th>sortOrder</th>\n",
              "      <th>appId</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Andrew Thomas</td>\n",
              "      <td>https://lh3.googleusercontent.com/a-/AOh14GiHd...</td>\n",
              "      <td>Update: After getting a response from the deve...</td>\n",
              "      <td>1</td>\n",
              "      <td>21</td>\n",
              "      <td>4.17.0.3</td>\n",
              "      <td>2020-04-05 22:25:57</td>\n",
              "      <td>According to our TOS, and the term you have ag...</td>\n",
              "      <td>2020-04-05 15:10:24</td>\n",
              "      <td>most_relevant</td>\n",
              "      <td>com.anydo</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Craig Haines</td>\n",
              "      <td>https://lh3.googleusercontent.com/-hoe0kwSJgPQ...</td>\n",
              "      <td>Used it for a fair amount of time without any ...</td>\n",
              "      <td>1</td>\n",
              "      <td>11</td>\n",
              "      <td>4.17.0.3</td>\n",
              "      <td>2020-04-04 13:40:01</td>\n",
              "      <td>It sounds like you logged in with a different ...</td>\n",
              "      <td>2020-04-05 15:11:35</td>\n",
              "      <td>most_relevant</td>\n",
              "      <td>com.anydo</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>steven adkins</td>\n",
              "      <td>https://lh3.googleusercontent.com/a-/AOh14GiXw...</td>\n",
              "      <td>Your app sucks now!!!!! Used to be good but no...</td>\n",
              "      <td>1</td>\n",
              "      <td>17</td>\n",
              "      <td>4.17.0.3</td>\n",
              "      <td>2020-04-01 16:18:13</td>\n",
              "      <td>This sounds odd! We are not aware of any issue...</td>\n",
              "      <td>2020-04-02 16:05:56</td>\n",
              "      <td>most_relevant</td>\n",
              "      <td>com.anydo</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Lars Panzerbjørn</td>\n",
              "      <td>https://lh3.googleusercontent.com/a-/AOh14Gg-h...</td>\n",
              "      <td>It seems OK, but very basic. Recurring tasks n...</td>\n",
              "      <td>1</td>\n",
              "      <td>192</td>\n",
              "      <td>4.17.0.2</td>\n",
              "      <td>2020-03-12 08:17:34</td>\n",
              "      <td>We do offer this option as part of the Advance...</td>\n",
              "      <td>2020-03-15 06:20:13</td>\n",
              "      <td>most_relevant</td>\n",
              "      <td>com.anydo</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Scott Prewitt</td>\n",
              "      <td>https://lh3.googleusercontent.com/-K-X1-YsVd6U...</td>\n",
              "      <td>Absolutely worthless. This app runs a prohibit...</td>\n",
              "      <td>1</td>\n",
              "      <td>42</td>\n",
              "      <td>4.17.0.2</td>\n",
              "      <td>2020-03-14 17:41:01</td>\n",
              "      <td>We're sorry you feel this way! 90% of the app ...</td>\n",
              "      <td>2020-03-15 23:45:51</td>\n",
              "      <td>most_relevant</td>\n",
              "      <td>com.anydo</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           userName  ... sentiment\n",
              "0     Andrew Thomas  ...         0\n",
              "1      Craig Haines  ...         0\n",
              "2     steven adkins  ...         0\n",
              "3  Lars Panzerbjørn  ...         0\n",
              "4     Scott Prewitt  ...         0\n",
              "\n",
              "[5 rows x 12 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V-155O-SFSqE"
      },
      "source": [
        "class_names = ['negative', 'neutral', 'positive']"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y3tY3ECJDPaz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "outputId": "39f541a9-d49f-47e5-ea56-67aa1fdc9f6a"
      },
      "source": [
        "# TODO: Q3. Plot the histogram of review sentiments, and show that it is now approximately balanced.\n",
        "data1 = df.values\n",
        "plt.hist(data1[:,-1])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([5028.,    0.,    0.,    0.,    0., 5042.,    0.,    0.,    0.,\n",
              "        5676.]),\n",
              " array([0.0, 0.2, 0.4, 0.6000000000000001, 0.8, 1.0, 1.2000000000000002,\n",
              "        1.4000000000000001, 1.6, 1.8, 2.0], dtype=object),\n",
              " <a list of 10 Patch objects>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9aHyGuTFgyPO"
      },
      "source": [
        "## Data Preprocessing\n",
        "\n",
        "Let's now load a pre-trained BERT model and the corresponding tokenizer, which converts text data into tokens. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E7Mj-0ne--5t"
      },
      "source": [
        "PRE_TRAINED_MODEL_NAME = 'bert-base-cased'"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QcYSqB13NUGe"
      },
      "source": [
        "! pip install torch>=1.2.0 transformers>=2.5.0"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H3AfJSZ8NNLF"
      },
      "source": [
        "import transformers\n",
        "from transformers import BertTokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CfrSbwTQ-wi_"
      },
      "source": [
        "Let's see how tokenization works. Here is the test sentence. Convert into tokens using the `tokenizer.tokenize` and `tokenizer.convert_tokens_to_ids` methods.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HZMitwrqm2eb"
      },
      "source": [
        "sample_txt = 'Every day feels like the same during the lock down.'"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iTFhpHpsoWO7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c1a2bcc-b669-480d-93dd-1af8aeac4291"
      },
      "source": [
        "# TODO: Q4. Print the tokens and token ids of the sample text above.\n",
        "sample_token = tokenizer.tokenize(sample_txt)\n",
        "sample_token_id = tokenizer.convert_tokens_to_ids(sample_token)\n",
        "print(sample_token,sample_token_id)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Every', 'day', 'feels', 'like', 'the', 'same', 'during', 'the', 'lock', 'down', '.'] [4081, 1285, 5115, 1176, 1103, 1269, 1219, 1103, 5842, 1205, 119]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W9ap7jdL0LYU"
      },
      "source": [
        "BERT has special tokens for sentence separators \\[SEP\\] and unknown words \\[UNK\\]. This can be done using the [`encode_plus()`](https://huggingface.co/transformers/main_classes/tokenizer.html#transformers.PreTrainedTokenizer.encode_plus) method, which takes the test sentence and encodes it into `input_ids`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vea9edaaxSPO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d225abe-610c-4638-eebe-5da01544e194"
      },
      "source": [
        "encoding = tokenizer.encode_plus(\n",
        "  sample_txt,\n",
        "  max_length=32,\n",
        "  add_special_tokens=True, # Add '[CLS]' and '[SEP]'\n",
        "  return_token_type_ids=False,\n",
        "  pad_to_max_length=True,\n",
        "  return_attention_mask=True,\n",
        "  return_tensors='pt',  # Return PyTorch tensors\n",
        ")\n",
        "\n",
        "encoding.keys()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2074: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['input_ids', 'attention_mask'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sS69c8WvdOED"
      },
      "source": [
        "The token ids are now stored in a Tensor and padded to a length of 32:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YzBmcOla0yQR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81641159-f86a-47d4-da2e-ad322d318b91"
      },
      "source": [
        "print(len(encoding['input_ids'][0]))\n",
        "encoding['input_ids'][0]"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "32\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 101, 4081, 1285, 5115, 1176, 1103, 1269, 1219, 1103, 5842, 1205,  119,\n",
              "         102,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "           0,    0,    0,    0,    0,    0,    0,    0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "itAyVPsNdyc1"
      },
      "source": [
        "The attention mask has the same length:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wiv5LLiw03Ox",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad67ab69-5573-4e19-f2da-182b12353732"
      },
      "source": [
        "print(len(encoding['attention_mask'][0]))\n",
        "encoding['attention_mask']"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "32\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m1RvhC4jNHHy"
      },
      "source": [
        "Use the `tokenizer.convert_ids_to_tokens` method to invert the encoded token ids (the above tensor of length 32) and visualize the sentence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IagGoafKLUwW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "823f73c6-95c6-409b-f95f-2c370647fb09"
      },
      "source": [
        "# TODO: Q5. Invert the encoded token ids.\n",
        "tokens = tokenizer.convert_ids_to_tokens(encoding['input_ids'][0])\n",
        "print(tokens)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['[CLS]', 'Every', 'day', 'feels', 'like', 'the', 'same', 'during', 'the', 'lock', 'down', '.', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oW6ajl30t6du"
      },
      "source": [
        "Most reviews in the dataset contain less than around 120 tokens, but let us choose a maximum length of 160."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t7xSmJtLuoxW"
      },
      "source": [
        "MAX_LEN = 160"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XvvcoU6nurHy"
      },
      "source": [
        "# Building the dataset\n",
        "\n",
        "Let's now create a dataset using the tokenizer. Here is some code that does this:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E2BPgRJ7YBK0"
      },
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class GPReviewDataset(Dataset):\n",
        "\n",
        "  def __init__(self, reviews, targets, tokenizer, max_len):\n",
        "    self.reviews = reviews\n",
        "    self.targets = targets\n",
        "    self.tokenizer = tokenizer\n",
        "    self.max_len = max_len\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.reviews)\n",
        "  \n",
        "  def __getitem__(self, item):\n",
        "    review = str(self.reviews[item])\n",
        "    target = self.targets[item]\n",
        "\n",
        "    encoding = self.tokenizer.encode_plus(\n",
        "      review,\n",
        "      add_special_tokens=True,\n",
        "      max_length=self.max_len,\n",
        "      return_token_type_ids=False,\n",
        "      pad_to_max_length=True,\n",
        "      return_attention_mask=True,\n",
        "      return_tensors='pt',\n",
        "    )\n",
        "\n",
        "    return {\n",
        "      'review_text': review,\n",
        "      'input_ids': encoding['input_ids'].flatten(),\n",
        "      'attention_mask': encoding['attention_mask'].flatten(),\n",
        "      'targets': torch.tensor(target, dtype=torch.long)\n",
        "    }"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x2uwsvCYqDJK"
      },
      "source": [
        "The tokenizer is doing most of the heavy lifting for us. We also return the review texts, so it'll be easier to evaluate the predictions from our model. Let's split the data into 90-5-5 train-validation-test."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B-vWzoo81dvO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ed26c39-1ca5-4319-8e8c-0cc2e391bb00"
      },
      "source": [
        "# TODO: Q6. Create three data frames: df_train, df_val, df_train as above and print their shapes.\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "df\n",
        "df_train, df_val, df_test = np.split(df.sample(frac=1, random_state=42), [ int(.9*len(df)), int(.95*len(df))])\n",
        "df_train.shape, df_val, df_test\n",
        "print(\"train shape : \", df_train.shape)\n",
        "print(\"val shape   : \",df_val.shape)\n",
        "print(\"test shape  : \",df_test.shape)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train shape :  (14171, 12)\n",
            "val shape   :  (787, 12)\n",
            "test shape  :  (788, 12)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J4tQ1x-vqNab"
      },
      "source": [
        "We also need to create a couple of data loaders:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KEGqcvkuOuTX"
      },
      "source": [
        "def create_data_loader(df, tokenizer, max_len, batch_size):\n",
        "  ds = GPReviewDataset(\n",
        "    reviews=df.content.to_numpy(),\n",
        "    targets=df.sentiment.to_numpy(),\n",
        "    tokenizer=tokenizer,\n",
        "    max_len=max_len\n",
        "  )\n",
        "\n",
        "  return DataLoader(\n",
        "    ds,\n",
        "    batch_size=batch_size,\n",
        "    num_workers=4\n",
        "  )"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vODDxMKsPHqI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6fde205-724f-4c8c-cd5c-b82cff6fa2c0"
      },
      "source": [
        "BATCH_SIZE = 16\n",
        "\n",
        "train_data_loader = create_data_loader(df_train, tokenizer, MAX_LEN, BATCH_SIZE)\n",
        "val_data_loader = create_data_loader(df_val, tokenizer, MAX_LEN, BATCH_SIZE)\n",
        "test_data_loader = create_data_loader(df_test, tokenizer, MAX_LEN, BATCH_SIZE)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A6dlOptwqlhF"
      },
      "source": [
        "Let's have a look at an example batch from our training data loader:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y93ldSN47FeT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "018b206d-86ab-4aca-cf44-146b452e2554"
      },
      "source": [
        "import torch\n",
        "\n",
        "data = next(iter(train_data_loader))\n",
        "data.keys()\n",
        "print(data['input_ids'].shape)\n",
        "print(data['attention_mask'].shape)\n",
        "print(data['targets'].shape)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2074: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2074: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2074: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2074: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "torch.Size([16, 160])\n",
            "torch.Size([16, 160])\n",
            "torch.Size([16])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "440Nd31VTHER"
      },
      "source": [
        "Let's now load the basic [BertModel](https://huggingface.co/transformers/model_doc/bert.html#bertmodel) and build our sentiment classifier on top of it. Load the model using:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0P41FayISNRI"
      },
      "source": [
        "from transformers import BertModel\n",
        "\n",
        "bert_model = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aFE7YSbFdY4t"
      },
      "source": [
        "And encode our sample text:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s1aoFxbQSn15"
      },
      "source": [
        "bm = bert_model(\n",
        "  input_ids=encoding['input_ids'], \n",
        "  attention_mask=encoding['attention_mask']\n",
        ")\n",
        "last_hidden_state = bm[0]\n",
        "pooled_output = bm[1]"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mLLu8zmqbaHV"
      },
      "source": [
        "The `last_hidden_state` is the sequence of hidden states of the last layer of the model. The `pooled_output` can be thought of as a summary of the content in the test sentence. Try printing out the sizes of `last_hidden_state` and `pooled_output`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mUJHXNpIbcci",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b38ebc31-b7b2-4d5b-a70e-f6b30b675023"
      },
      "source": [
        "# TODO: Q7. Print the sizes of the hidden states and the pooled output.\n",
        "# print(last_hidden_state.)\n",
        "print(\"size of last_hidden_state : \", last_hidden_state.shape)\n",
        "print(\"size of pooled_output     : \",pooled_output.shape)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "size of last_hidden_state :  torch.Size([1, 32, 768])\n",
            "size of pooled_output     :  torch.Size([1, 768])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0o_NiS3WgOFf"
      },
      "source": [
        "We can use all of this knowledge to create a classifier that uses the BERT model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m_mRflxPl32F"
      },
      "source": [
        "from torch import nn, optim\n",
        "\n",
        "class SentimentClassifier(nn.Module):\n",
        "\n",
        "  def __init__(self, n_classes):\n",
        "    super(SentimentClassifier, self).__init__()\n",
        "    self.bert = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
        "    self.drop = nn.Dropout(p=0.3)\n",
        "    self.out = nn.Linear(self.bert.config.hidden_size, n_classes)\n",
        "  \n",
        "  def forward(self, input_ids, attention_mask):\n",
        "    _, pooled_output = self.bert(\n",
        "      input_ids=input_ids,\n",
        "      attention_mask=attention_mask,\n",
        "      return_dict = False\n",
        "\n",
        "    )\n",
        "    output = self.drop(pooled_output)\n",
        "    return self.out(output)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJg8m3NQJahc"
      },
      "source": [
        "Note that our sentiment classifier takes the BERT backbone and adds a dropout layer (for regularization) and a linear dense layer, which we train using cross-entropy. Let's create an instance and move it to the GPU:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i0yQnuSFsjDp"
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = SentimentClassifier(len(class_names))\n",
        "model = model.to(device)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VCPCFDLlKIQd"
      },
      "source": [
        "We'll move the example batch of our training data to the GPU:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mz7p__CqdaMO"
      },
      "source": [
        "input_ids = data['input_ids'].to(device)\n",
        "attention_mask = data['attention_mask'].to(device)\n",
        "# print(input_ids,attention_mask)\n"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hr1EgkEtKOIB"
      },
      "source": [
        "To get the predicted probabilities from our trained model, we'll apply the softmax function to the outputs:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2rTCj46Zamry",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "949ce1b0-643b-4311-8e61-e6a885400727"
      },
      "source": [
        "from torch.nn import functional as F\n",
        "outputs = model(\n",
        "\n",
        " input_ids = input_ids,\n",
        "\n",
        " attention_mask = attention_mask,\n",
        "\n",
        " )\n",
        "# print(outputs)\n",
        "F.softmax(outputs, dim=1)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.1993, 0.3537, 0.4470],\n",
              "        [0.1534, 0.4638, 0.3827],\n",
              "        [0.1646, 0.4920, 0.3433],\n",
              "        [0.2105, 0.4637, 0.3259],\n",
              "        [0.1565, 0.4319, 0.4116],\n",
              "        [0.2599, 0.5200, 0.2201],\n",
              "        [0.2482, 0.4912, 0.2605],\n",
              "        [0.1813, 0.3353, 0.4834],\n",
              "        [0.1479, 0.4530, 0.3991],\n",
              "        [0.1774, 0.4380, 0.3846],\n",
              "        [0.1831, 0.5139, 0.3030],\n",
              "        [0.3157, 0.3235, 0.3608],\n",
              "        [0.2217, 0.4190, 0.3593],\n",
              "        [0.1557, 0.5244, 0.3199],\n",
              "        [0.2179, 0.4252, 0.3569],\n",
              "        [0.2568, 0.4625, 0.2807]], device='cuda:0', grad_fn=<SoftmaxBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g9xikRdtRN1N"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76g7FV85H-T8"
      },
      "source": [
        "To train the model, we will use the AdamW optimizer and a linear learning-rate scheduler with no warmup steps, along with the cross-entropy loss. Five epochs (full passes through the training data should be enough) should be enough, but you can experiment with more epochs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5v-ArJ2fCCcU"
      },
      "source": [
        "EPOCHS = 5\n",
        "from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False)\n",
        "total_steps = len(train_data_loader) * EPOCHS\n",
        "\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "  optimizer,\n",
        "  num_warmup_steps=0,\n",
        "  num_training_steps=total_steps\n",
        ")\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss().to(device)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A8522g7JIu5J"
      },
      "source": [
        "\n",
        "Let's continue with writing a helper function for training our model for one epoch:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bzl9UhuNx1_Q"
      },
      "source": [
        "def train_epoch(\n",
        "  model, \n",
        "  data_loader, \n",
        "  loss_fn, \n",
        "  optimizer, \n",
        "  device, \n",
        "  scheduler, \n",
        "  n_examples\n",
        "):\n",
        "  model = model.train()\n",
        "\n",
        "  losses = []\n",
        "  correct_predictions = 0\n",
        "  \n",
        "  for d in data_loader:\n",
        "    # TODO Q8. Complete the incomplete code snippets below to finish training.\n",
        "    \n",
        "    input_ids = d[\"input_ids\"].to(device)\n",
        "    attention_mask = d[\"attention_mask\"].to(device)\n",
        "    targets = d[\"targets\"].to(device)\n",
        "\n",
        "    outputs = model(\n",
        "      input_ids=input_ids,\n",
        "      attention_mask=attention_mask\n",
        "    )\n",
        "\n",
        "    _, preds = torch.max(outputs, dim=1)\n",
        "    loss = loss_fn(outputs, targets)\n",
        "\n",
        "    correct_predictions += torch.sum(preds == targets)\n",
        "    losses.append(loss.item())\n",
        "\n",
        "    loss.backward()\n",
        "    nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "    optimizer.step()\n",
        "    scheduler.step()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "  return correct_predictions.double() / n_examples, np.mean(losses)"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E4PniYIte0fr"
      },
      "source": [
        "Let's write another function that helps us evaluate the model on a given data loader."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CXeRorVGIKre"
      },
      "source": [
        "def eval_model(model, data_loader, loss_fn, device, n_examples):\n",
        "  model = model.eval()\n",
        "\n",
        "  # TODO: Q9. Reproduce the above code but only evaluate the model (without any weight updates).\n",
        "  losses = []\n",
        "  correct_predictions = 0\n",
        "  with torch.no_grad():\n",
        "    for d in data_loader:\n",
        "      input_ids = d[\"input_ids\"].to(device)\n",
        "      attention_mask = d[\"attention_mask\"].to(device)\n",
        "      targets = d[\"targets\"].to(device)\n",
        "      outputs = model(\n",
        "        input_ids=input_ids,\n",
        "        attention_mask=attention_mask\n",
        "      )\n",
        "      _, preds = torch.max(outputs, dim=1)\n",
        "      loss = loss_fn(outputs, targets)\n",
        "      correct_predictions += torch.sum(preds == targets)\n",
        "      losses.append(loss.item())\n",
        "  return correct_predictions.double() / n_examples, np.mean(losses)"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_rdSDBHhhCh"
      },
      "source": [
        "Using those two, we can write our training loop."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VgdtRRghVPZt"
      },
      "source": [
        "%matplotlib inline\n",
        "from scipy import stats\n",
        "import string\n",
        "import re\n",
        "import datetime as dt\n",
        "from collections import defaultdict\n"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1zhHoFNsxufs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5fff2d66-cdc9-4e54-9567-c16ed9fccc41"
      },
      "source": [
        "%%time\n",
        "\n",
        "history = defaultdict(list)\n",
        "best_accuracy = 0\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "\n",
        "  print(f'Epoch {epoch + 1}/{EPOCHS}')\n",
        "  print('-' * 10)\n",
        "\n",
        "  # TODO: Q10. Complete the code below to track train and test accuracy.losses\n",
        "\n",
        "  train_acc, train_loss = train_epoch(model,train_data_loader,loss_fn, optimizer, device, scheduler,len(df_train))\n",
        "\n",
        "  print(f'Train loss {train_loss} accuracy {train_acc}')\n",
        "\n",
        "  val_acc, val_loss = eval_model(model,val_data_loader, loss_fn, device,len(df_val))\n",
        "\n",
        "  print(f'Val   loss {val_loss} accuracy {val_acc}')\n",
        "  print()\n",
        "\n",
        "  history['train_acc'].append(train_acc)\n",
        "  history['train_loss'].append(train_loss)\n",
        "  history['val_acc'].append(val_acc)\n",
        "  history['val_loss'].append(val_loss)\n",
        "\n",
        "  if val_acc > best_accuracy:\n",
        "    torch.save(model.state_dict(), 'best_model_state.bin')\n",
        "    best_accuracy = val_acc"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2074: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2074: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2074: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2074: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.609094206840809 accuracy 0.7405969938607014\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2074: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2074: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2074: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2074: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Val   loss 0.5418225899338722 accuracy 0.758576874205845\n",
            "\n",
            "Epoch 2/5\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2074: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2074: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2074: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2074: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.3362440642954374 accuracy 0.8764377954978477\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2074: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2074: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2074: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2074: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Val   loss 0.4462189966440201 accuracy 0.8475222363405337\n",
            "\n",
            "Epoch 3/5\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2074: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2074: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2074: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2074: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.19266133641027353 accuracy 0.9408651471314657\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2074: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2074: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2074: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2074: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Val   loss 0.5540806541219354 accuracy 0.8640406607369759\n",
            "\n",
            "Epoch 4/5\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2074: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2074: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2074: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2074: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.12476907593264343 accuracy 0.9631642085950181\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2074: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2074: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2074: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2074: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Val   loss 0.6033495782781393 accuracy 0.8691232528589581\n",
            "\n",
            "Epoch 5/5\n",
            "----------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2074: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2074: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2074: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2074: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss 0.0937501681568737 accuracy 0.9733963728741797\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2074: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2074: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2074: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2074: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Val   loss 0.638042804999277 accuracy 0.8729351969504447\n",
            "\n",
            "CPU times: user 23min 46s, sys: 14min 57s, total: 38min 43s\n",
            "Wall time: 39min 20s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4r8-5zWsiVur"
      },
      "source": [
        "Note that we're storing the best model, indicated by the highest validation accuracy.\n",
        "\n",
        "Plot train and validation accuracy as a function of epoch count."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-FWG7kBm372V",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "12c49d3d-84d0-4911-dff3-a73ad322724c"
      },
      "source": [
        "# TODO: Q11. Plot train/validation accuracies.\n",
        "plt.plot(history['train_acc'], label='train accuracy')\n",
        "plt.plot(history['val_acc'], label='validation accuracy')\n",
        "plt.title('Training history')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "plt.ylim([0, 1]);"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZsHqkLAuf8pv"
      },
      "source": [
        "You might try to fine-tune the parameters (learning rate, batch size) a bit more if accuracy is not good enough.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U3HZb3NWFtFf"
      },
      "source": [
        "## Evaluation\n",
        "\n",
        "So how good is our model on predicting sentiment?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mdQ7-ylCj8Gd"
      },
      "source": [
        "We'll define a helper function to get the predictions from our model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EgR6MuNS8jr_"
      },
      "source": [
        "def get_predictions(model, data_loader):\n",
        "  model = model.eval()\n",
        "  \n",
        "  review_texts = []\n",
        "  predictions = []\n",
        "  prediction_probs = []\n",
        "  real_values = []\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for d in data_loader:\n",
        "\n",
        "      texts = d[\"review_text\"]\n",
        "      input_ids = d[\"input_ids\"].to(device)\n",
        "      attention_mask = d[\"attention_mask\"].to(device)\n",
        "      targets = d[\"targets\"].to(device)\n",
        "\n",
        "      outputs = model(\n",
        "        input_ids=input_ids,\n",
        "        attention_mask=attention_mask\n",
        "      )\n",
        "      _, preds = torch.max(outputs, dim=1)\n",
        "\n",
        "      probs = F.softmax(outputs, dim=1)\n",
        "\n",
        "      review_texts.extend(texts)\n",
        "      predictions.extend(preds)\n",
        "      prediction_probs.extend(probs)\n",
        "      real_values.extend(targets)\n",
        "\n",
        "  predictions = torch.stack(predictions).cpu()\n",
        "  prediction_probs = torch.stack(prediction_probs).cpu()\n",
        "  real_values = torch.stack(real_values).cpu()\n",
        "  return review_texts, predictions, prediction_probs, real_values"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dkbnBTI7kd_y"
      },
      "source": [
        "This is similar to the evaluation function, except that we're storing the text of the reviews and the predicted probabilities (by applying the softmax on the model outputs):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zHdPZr60-0c_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4f1ac97-3b87-4605-d728-9a3abb0362a5"
      },
      "source": [
        "y_review_texts, y_pred, y_pred_probs, y_test = get_predictions(\n",
        "  model,\n",
        "  test_data_loader\n",
        ")"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2074: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2074: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2074: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2074: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rFAekw3mmWUi"
      },
      "source": [
        "Let us compare true sentiment vs predicted sentiment by plotting a confusion matrix of `y_test` vs `y_pred`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6d1qxsc__DTh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "outputId": "fd6bdf69-a787-4207-c281-b0a1b4426fde"
      },
      "source": [
        "# TODO. Q12. Plot the 3x3 confusion matrix and show that the model finds it a bit difficult to classify neutral reviews.\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "\n",
        "def show_confusion_matrix(confusion_matrix):\n",
        "  heatmap = sns.heatmap(confusion_matrix, annot=True, fmt=\"d\")\n",
        "  heatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation=0, ha='right')\n",
        "  heatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), rotation=30, ha='right')\n",
        "  plt.ylabel('True label')\n",
        "  plt.xlabel('Predicted label');\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "df_cm = pd.DataFrame(cm, index=[0,1,2], columns=[0,1,2])\n",
        "show_confusion_matrix(df_cm)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7WL5pDmvFyaU"
      },
      "source": [
        "### Predicting on Raw Text\n",
        "\n",
        "Let's use our model to predict the sentiment of some raw text:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QEPi7zQRsDhH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39879b62-ce38-48a9-8f64-a4ada482bc05"
      },
      "source": [
        "review_text = \"I love Deep Learning! Best course evah!!!1!!\"\n"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "negative\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2074: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "et8xlDrKpH60"
      },
      "source": [
        "Use your trained model to predict the sentiment expressed in `review_text`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qr_t3rUksumr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2fac3a0c-5ebe-4aa1-d01a-f78a27a56db6"
      },
      "source": [
        "# TODO: Q13. Print the predicted sentiment in `review_text`.\n",
        "encodedReviewText = tokenizer.encode_plus(\n",
        "  review_text,\n",
        "  max_length=MAX_LEN,\n",
        "  add_special_tokens=True, # Add '[CLS]' and '[SEP]'\n",
        "  return_token_type_ids=False,\n",
        "  pad_to_max_length=True,\n",
        "  return_attention_mask=True,\n",
        "  return_tensors='pt',  # Return PyTorch tensors\n",
        ")\n",
        "input_ids = encodedReviewText['input_ids'].to(device)\n",
        "attention_mask = encodedReviewText['attention_mask'].to(device)\n",
        "output = model(input_ids, attention_mask)\n",
        "_, prediction = torch.max(output, dim=1)\n",
        "\n",
        "print(class_names[prediction])\n"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "negative\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2074: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wf39tauBa2V2"
      },
      "source": [
        "## References\n",
        "\n",
        "- [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)\n",
        "- [L11 Language Models - Alec Radford (OpenAI)](https://www.youtube.com/watch?v=BnpB3GrpsfM)\n",
        "- [The Illustrated BERT, ELMo, and co.](https://jalammar.github.io/illustrated-bert/)\n",
        "- [BERT Fine-Tuning Tutorial with PyTorch](https://mccormickml.com/2019/07/22/BERT-fine-tuning/)\n",
        "- [How to Fine-Tune BERT for Text Classification?](https://arxiv.org/pdf/1905.05583.pdf)\n",
        "- [Huggingface Transformers](https://huggingface.co/transformers/)\n",
        "- [BERT Explained: State of the art language model for NLP](https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270)"
      ]
    }
  ]
}